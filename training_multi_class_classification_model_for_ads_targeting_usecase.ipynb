{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503077811e70"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e885ac09bc73"
      },
      "source": [
        "# Train a multi-class classification model for ads-targeting\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00f095e91b2d"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to build a machine learning model for an ads-targeting use case. Ads-targeting is an advertisement technique where chosen or tailor-made ads are shown to the customers based on their past behavior and preferences. Targeted ads are meant to reach specific customers based on demographics, psychographics, behavior, and other second-order activities that are learned usually through data collected from the customers.\n",
        "\n",
        "*Note: If you are using [Vertex AI Workbench managed notebooks](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-instance) instance use the `TensorFlow 2 (Local)` kernel. Some components of this notebook may not work in other notebook environments.*\n",
        "\n",
        "Learn more about [Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction) and [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bea2b6e9b25"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to collect data from BigQuery, preprocess it, and train a multi-class classification model on an e-commerce dataset.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- BigQuery\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Fetch the required data from BigQuery\n",
        "- Preprocess the data\n",
        "- Train a TensorFlow (>=2.4) classification model\n",
        "- Evaluate the loss for the trained model\n",
        "- Automate the notebook execution using the executor feature\n",
        "- Save the model to a Cloud Storage path\n",
        "- Clean up the created resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d623e6dfa3"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "This tutorial uses the `looker-private-demo.ecomm` dataset in BigQuery. The dataset consists of information about various advertisement campaigns including the demographics of users who have clicked and made some purchases after seeing the ads. For this tutorial, the top three campaigns from the USA are selected from this dataset and user information for those who have made purchases shall be used to train a model with the campaigns as the classes. The idea is to see if the advertisement and the user data can be used to identify which campaign is best-suited for the user.\n",
        "\n",
        "The dataset can be accessed by pinning the `looker-private-demo` project in BigQuery. If you are using Vertex AI Workbench managed notebooks instance, instead of going to the BigQuery user interface, this process can be performed from the JupyterLab user interface. Vertex AI Workbench managed notebooks instances support browsing through the datasets and tables from BigQuery through its BigQuery integration.\n",
        "\n",
        "<img src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/images/Bigquery_UI_new.PNG?raw=1\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee02650bb7fd"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery\n",
        "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DouUvNOkXT8"
      },
      "source": [
        "### Install additional packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayt1jhFXkXT9"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet pandas-gbq \\\n",
        "                                 'google-cloud-bigquery[bqstorage,pandas]' \\\n",
        "                                 tensorflow \\\n",
        "                                 scikit-learn \\\n",
        "                                 numpy \\\n",
        "                                 protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install fastapi\n",
        "! pip3 install kaleido\n",
        "! pip3 install python-multipart\n",
        "! pip3 install uvicorn"
      ],
      "metadata": {
        "id": "HhteACO8ggu_",
        "outputId": "6d12ca85-27aa-4b01-a2e9-e659470d9159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n",
            "Collecting starlette<0.33.0,>=0.29.0 (from fastapi)\n",
            "  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.33.0,>=0.29.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi) (1.2.0)\n",
            "Installing collected packages: typing-extensions, starlette, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.108.0 starlette-0.32.0.post1 typing-extensions-4.9.0\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kaleido-0.2.1\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m738.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-multipart\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed python-multipart-0.0.6\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m969.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.9.0)\n",
            "Installing collected packages: h11, uvicorn\n",
            "Successfully installed h11-0.14.0 uvicorn-0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3",
        "outputId": "b7581e9a-4716-454c-b76f-1b55a7fef31f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gcloud auth login"
      ],
      "metadata": {
        "id": "YeH5p7IvhM0c",
        "outputId": "8bf7c189-6e4f-4998-d19c-5b7c618349c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=jq8o4Armxoq0taJOZ2aIYwPLTOY2vx&prompt=consent&access_type=offline&code_challenge=VK4ByJ-5NMC1yCutkxlkyaAMpwjTUWVZ8xZkuIDtG2o&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AfJohXnyZTLFpuYnqBFr-gqLlMQ9cCVpyIpjIleypt82WCB6zGpE_dz1oHnkGKssjCdYXw\n",
            "\n",
            "You are now logged in as [t.hsuanhsieh@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gcloud config set project GenAI_Exercise"
      ],
      "metadata": {
        "id": "gy-DEWlGhuGF",
        "outputId": "da15ad31-9142-4da5-81c1-589cff4598f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [GenAI_Exercise] or it does not exist.\n",
            "Are you sure you wish to set property [core/project] to GenAI_Exercise?\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.config.set) The project property must be set to a valid project ID, not the project name [GenAI_Exercise]\n",
            "To set your project, run:\n",
            "\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "\n",
            "or to unset it, run:\n",
            "\n",
            "  $ gcloud config unset project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud projects list"
      ],
      "metadata": {
        "id": "UfS27zvbhUjJ",
        "outputId": "03199a60-738c-466a-f495-2c137fc54009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.projects.list) You do not currently have an active account selected.\n",
            "Please run:\n",
            "\n",
            "  $ gcloud auth login\n",
            "\n",
            "to obtain new credentials.\n",
            "\n",
            "If you have already logged in with a different account, run:\n",
            "\n",
            "  $ gcloud config set account ACCOUNT\n",
            "\n",
            "to select an already authenticated account to use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "$ gcloud auth login"
      ],
      "metadata": {
        "id": "1NII96IXiKdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1",
        "outputId": "4ded7c91-0337-439b-a08d-23aeb136f6f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [GenAI_Exercise] or it does not exist.\n",
            "Are you sure you wish to set property [core/project] to GenAI_Exercise?\n",
            "\n",
            "Do you want to continue (Y/n)?  "
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"GenAI_Exercise\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Set the region\n",
        "\n",
        "**Optional**: Update the 'REGION' variable to specify the region that you want to use. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsN5NJKSu-GU"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2b04f364669"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a UUID for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ee95826661"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "To authenticate your Google Cloud account, follow the instructions for your Jupyter environment:\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "<br>You are already authenticated.\n",
        "\n",
        "**2. Local JupyterLab instance**\n",
        "<br>Uncomment and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab**\n",
        "<br>Uncomment and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmnMD2MjkXUJ"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqtZRqDEkXUJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "from google.cloud.bigquery import Client\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abd63bb1a85"
      },
      "source": [
        "## Tutorial\n",
        "\n",
        "### Fetch the data from BigQuery\n",
        "If you are using ***Vertex AI Workbench managed notebooks instance***, below cell which starts with \"#@bigquery\" will be a SQL Query. If you are using Vertex AI Workbench user managed notebooks instance or Colab it will be a markdown cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a25861cf833"
      },
      "source": [
        "#@bigquery\n",
        "\n",
        "WITH\n",
        "  traindata AS (\n",
        "  SELECT\n",
        "    b.* EXCEPT(ad_event_id,\n",
        "      user_id),\n",
        "    c.* EXCEPT(id),\n",
        "    d.* EXCEPT(keyword_id,\n",
        "      ad_id),\n",
        "    a.amount,\n",
        "    a.device_type,\n",
        "    e.name\n",
        "  FROM\n",
        "    `looker-private-demo.ecomm.ad_events` a\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      ad_event_id,\n",
        "      user_id,\n",
        "      state,\n",
        "      os,\n",
        "      browser\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.events`\n",
        "    WHERE\n",
        "      event_type=\"Purchase\"\n",
        "      AND country=\"USA\") b\n",
        "  ON\n",
        "    a.id = b.ad_event_id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      id,\n",
        "      gender,\n",
        "      age\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.users`) c\n",
        "  ON\n",
        "    b.user_id = c.id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      keyword_id,\n",
        "      ad_id,\n",
        "      cpc_bid_amount,\n",
        "      bidding_strategy_type,\n",
        "      quality_score,\n",
        "      keyword_match_type\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.keywords`\n",
        "    WHERE\n",
        "      cpc_bid_amount <= 3000) d\n",
        "  ON\n",
        "    a.keyword_id = d.keyword_id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      ad_id,\n",
        "      name\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.ad_groups`) e\n",
        "  ON\n",
        "    d.ad_id = e.ad_id )\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  traindata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923fdd823683"
      },
      "source": [
        "If you are using Vertex AI Workbench managed notebooks instance, once the results from BigQuery are displayed in the above cell, click the **Query and load as DataFrame** button and execute the generated code stub to fetch the data into the current notebook as a dataframe.\n",
        "\n",
        "*Note: By default the data is loaded into a `df` variable, though this can be changed before executing the cell if required.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d44a10b6884"
      },
      "outputs": [],
      "source": [
        "client = Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8b5112f231f"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"WITH traindata AS (\n",
        "SELECT b.* except(ad_event_id, user_id), c.* except(id), d.* except(keyword_id, ad_id), a.amount, a.device_type, e.name\n",
        "FROM `looker-private-demo.ecomm.ad_events` a\n",
        "JOIN\n",
        "(SELECT ad_event_id, user_id, state, os, browser from `looker-private-demo.ecomm.events` WHERE event_type=\"Purchase\" AND country=\"USA\") b\n",
        "ON a.id = b.ad_event_id\n",
        "JOIN\n",
        "(SELECT id, gender, age FROM `looker-private-demo.ecomm.users`) c\n",
        "ON b.user_id = c.id\n",
        "JOIN\n",
        "(SELECT keyword_id, ad_id, cpc_bid_amount, bidding_strategy_type, quality_score, keyword_match_type FROM `looker-private-demo.ecomm.keywords`\n",
        "WHERE cpc_bid_amount <= 3000) d\n",
        "ON a.keyword_id = d.keyword_id\n",
        "JOIN\n",
        "(SELECT ad_id, name FROM `looker-private-demo.ecomm.ad_groups`) e\n",
        "ON d.ad_id = e.ad_id\n",
        ")\n",
        "SELECT * FROM traindata\"\"\"\n",
        "job = client.query(query)\n",
        "df = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f69bea65019"
      },
      "source": [
        "### Preprocess the data\n",
        "Select the necessary columns from the e-commerce data and divide them based on their type (numerical/categorical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cea2c44c50b"
      },
      "outputs": [],
      "source": [
        "target = \"name\"\n",
        "categ_cols = [\n",
        "    \"state\",\n",
        "    \"os\",\n",
        "    \"browser\",\n",
        "    \"gender\",\n",
        "    \"bidding_strategy_type\",\n",
        "    \"keyword_match_type\",\n",
        "    \"device_type\",\n",
        "]\n",
        "num_cols = [\"age\", \"cpc_bid_amount\", \"quality_score\", \"amount\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace612851261"
      },
      "source": [
        "#### Select top three campaigns\n",
        "From the current dataset, only the top three campaigns will be chosen to target the users. All the relevant information about the advertisement and the user who purchased an item after seeing the advertisement is available in the dataframe already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7282fbab4586"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"name\"].isin([\"Tops & Tees\", \"Active\", \"Accessories\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89106348ffe"
      },
      "source": [
        "#### Encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6cd256f455a"
      },
      "outputs": [],
      "source": [
        "df[\"name\"] = df[\"name\"].map({\"Tops & Tees\": 0, \"Active\": 1, \"Accessories\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2d5338b1b95"
      },
      "source": [
        "#### One-hot encode the categorical variables\n",
        "After one-hot encoding, the first level-column is dropped to avoid the [dummy-variable trap](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) scenario. This process is called *dummy-encoding*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d57706df2441"
      },
      "outputs": [],
      "source": [
        "def encode_cols(data, col):\n",
        "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
        "    categ = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
        "    # Adding the results to the master dataframe\n",
        "    data = pd.concat([data, categ], axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# dummy-encode the categorical fields\n",
        "for i in categ_cols:\n",
        "    df = encode_cols(df, i)\n",
        "    df.drop(columns=[i], inplace=True)\n",
        "\n",
        "# check the data's shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abf027eda2d"
      },
      "source": [
        "#### Split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0072d44b6163"
      },
      "outputs": [],
      "source": [
        "X = df[[i for i in df.columns if i != target]].copy()\n",
        "y = df[target].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=36\n",
        ")\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1a32b9d9640"
      },
      "source": [
        "#### Scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9620e04d8db2"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9f3ca04f91"
      },
      "source": [
        "### Train a TensorFlow model\n",
        "Convert the target column to a categorical encoded colum (one-hot encoded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebc87650ae13"
      },
      "outputs": [],
      "source": [
        "y_train_categ = to_categorical(y_train)\n",
        "y_test_categ = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd0014a7e1d"
      },
      "source": [
        "#### Define hyperparameters for model training\n",
        "\n",
        "*Note: Comment or remove the parameters from the following cell if they are provided already as an input parameter through the executor feature.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec020b36af20"
      },
      "outputs": [],
      "source": [
        "optimizer = \"sgd\"\n",
        "num_hidden_layers = 3\n",
        "num_neurons = [64, 128, 256]\n",
        "activ_func = [\"relu\", \"relu\", \"relu\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "406b731f576b"
      },
      "source": [
        "#### Define the architecture and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57839a187cf0"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# construct the neural network as per the defined parameters\n",
        "for i in range(num_hidden_layers):\n",
        "    if i == 0:\n",
        "        # add the input layer\n",
        "        model.add(\n",
        "            Dense(\n",
        "                num_neurons[i],\n",
        "                activation=activ_func[i],\n",
        "                input_shape=(X_train.shape[1],),\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # add the hidden layers\n",
        "        model.add(Dense(num_neurons[i], activation=activ_func[i]))\n",
        "\n",
        "# add the output layer\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "# compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ab12c34f258"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9321005e55ae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.asarray(X_train, dtype=np.float32)\n",
        "\n",
        "history = model.fit(X_train, y_train_categ, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08445f2cd02"
      },
      "source": [
        "### Evaluate the model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "599df6d2b9a4"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(X_test, dtype=np.float32)\n",
        "\n",
        "test_results = model.evaluate(X_test, y_test_categ, verbose=1)\n",
        "print(f\"Test results - Loss: {test_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ef0e081340"
      },
      "source": [
        "**Note:**  Please note that executor feature is available only in Vertex AI Workbench managed notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9769168778e8"
      },
      "source": [
        "### Automating the execution of the notebook using executor in Vertex AI Workbench managed notebooks instance\n",
        "\n",
        "If you are using Vertex AI Workbench managed notebooks instance, the executor can help you run a notebook file from start to end, with your choice of the environment, machine type, input parameters, and other characteristics. After setting up an execution, the notebook is executed as a job in Vertex AI custom training. Your jobs can be monitored from the <b>Notebook Executor</b> pane in the menu on the left.\n",
        "\n",
        "<img src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/images/executor.png?raw=1\"></img>\n",
        "\n",
        "Executor lets you choose the environment and machine type while automating the runs similar to Vertex AI training jobs without switching to the training jobs UI. Apart from the custom container that replicates the existing kernel by default, pre-built environments like TensorFlow Enterprise, PyTorch, and others can also be selected to run the notebook. Furthermore the required compute power can be specified by choosing from the list of machine types available, including GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf486c351581"
      },
      "source": [
        "### Scheduled runs on executor in Vertex AI Workbench managed notebooks instance\n",
        "\n",
        "Vertex AI Workbench managed noteboook runs can also be scheduled recurringly with the executor. To do so, select <b>Schedule-based recurring executions</b> as the run type instead of <b>One-time execution</b>. The frequency of the job and the time when it executes is provided when you create the execution.\n",
        "\n",
        "<img src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/images/executor_scheduled_runs2.png?raw=1\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6561007ac7f2"
      },
      "source": [
        "### Parameterizing the variables\n",
        "\n",
        "If you are using Vertex AI Workbench managed notebooks instance, executor lets you run a notebook with different sets of input parameters. If required, constants in the notebook can be treated as arguments to a function, and when you submit the execution, you can provide those constants as input parameters.\n",
        "\n",
        "<img src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/images/executor_input_parameters.png?raw=1\"></img>\n",
        "\n",
        "The hyperparameters defined during the model training step can be passed as arguments while submitting an execution. However, the values defined in the notebook itself should be removed or commented out before submitting the execution. Otherwise, the input parameters would just be overwritten by the values in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81db97fa3e8"
      },
      "source": [
        "### Save the model to a Cloud Storage path\n",
        "\n",
        "TensorFlow's `model.save()` method supports Cloud Storage paths as well as the local file paths while writing the model object to a file. It needs to be ensured that the service account being used to run this notebook has `write` permissions to the specified Cloud Storage path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dcefeb6a2d8"
      },
      "outputs": [],
      "source": [
        "GCS_PATH = BUCKET_URI + \"/path-to-save/\"\n",
        "model.save(GCS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c0ca2a517a"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab69210d5a8"
      },
      "outputs": [],
      "source": [
        "# Delete the Cloud Storage bucket\n",
        "\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}